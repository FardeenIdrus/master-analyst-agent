"""
Backtest Engine Module
Professional vectorized backtesting using VectorBT framework.

Features:
1. VectorBT Integration - 100x faster than loop-based backtesting
2. Transaction Costs - Commission (0.1%) + volume-based slippage
3. Walk-Forward Optimization - Prevents overfitting with OOS validation
4. Multiple Order Types - Market, limit, stop loss, trailing stops
5. Position Scaling - Partial entries/exits based on signal confidence

Walk-Forward Analysis:
- 5:1 in-sample/out-of-sample ratio
- Train on Year 1 → Test on Year 2
- Train on Years 1-2 → Test on Year 3
- Walk-Forward Efficiency (WFE): OOS/IS performance (>0.5 good, >0.7 excellent)
"""

import numpy as np
import pandas as pd
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple, Any
from enum import Enum
from datetime import datetime, timedelta
import warnings

import vectorbt as vbt

# Import PositionSizer for advanced position sizing
from position_sizer import PositionSizer
from performance_metrics import PerformanceAnalyser

# Suppress VectorBT warnings for cleaner output
warnings.filterwarnings('ignore', category=FutureWarning)


# =============================================================================
# ENUMS AND DATA CLASSES
# =============================================================================
class OrderType(Enum):
    """Order execution types."""
    MARKET = "market"
    LIMIT = "limit"
    STOP = "stop"
    STOP_LIMIT = "stop_limit"


@dataclass
class BacktestConfig:
    """Configuration for backtest parameters."""
    # Capital and sizing
    initial_capital: float = 100_000.0
    position_size_pct: float = 1.0  # 10% of capital per trade
    max_positions: int = 1  # Single position for now

    # Transaction costs
    commission_pct: float = 0.001  # 0.1% commission
    slippage_pct: float = 0.0005  # 0.05% base slippage
    volume_impact: float = 0.1  # Additional slippage based on volume

    # Risk management
    use_stop_loss: bool = True
    stop_loss_atr_mult: float = 2.0  # Stop loss = 2x ATR
    use_take_profit: bool = True
    take_profit_atr_mult: float = 3.0  # Take profit = 3x ATR
    use_trailing_stop: bool = False
    trailing_stop_pct: float = 0.05  # 5% trailing stop

    # Walk-forward settings
    walk_forward_ratio: float = 5.0  # 5:1 in-sample to out-of-sample
    min_train_periods: int = 252  # Minimum 1 year training
    min_test_periods: int = 63  # Minimum 3 months testing

    # Advanced position sizing (PositionSizer integration)
    use_position_sizer: bool = True  # Use PositionSizer for dynamic sizing
    target_volatility: float = 0.30  # 30% annualized target volatility (matches typical stock vol)
    max_portfolio_heat: float = 0.10  # 10% total capital at risk
    max_per_trade_risk: float = 0.02  # 2% per trade
    kelly_fraction: float = 0.25  # Quarter-Kelly for safety


@dataclass
class TradeResult:
    """Result of a single trade."""
    entry_date: datetime
    exit_date: datetime
    entry_price: float
    exit_price: float
    direction: str  # 'long' or 'short'
    size: float
    pnl: float
    pnl_pct: float
    duration_days: int
    exit_reason: str  # 'signal', 'stop_loss', 'take_profit', 'trailing_stop'


@dataclass
class BacktestResult:
    """Complete backtest results."""
    # Performance metrics
    total_return: float
    annual_return: float
    sharpe_ratio: float
    sortino_ratio: float
    max_drawdown: float
    max_drawdown_duration: int  # days
    calmar_ratio: float

    # Trade statistics
    total_trades: int
    winning_trades: int
    losing_trades: int
    win_rate: float
    avg_win: float
    avg_loss: float
    profit_factor: float
    avg_trade_duration: float

    # Risk metrics
    volatility: float
    var_95: float  # Value at Risk 95%
    cvar_95: float  # Conditional VaR 95%

    # Equity curve
    equity_curve: pd.Series = field(default_factory=pd.Series)
    drawdown_curve: pd.Series = field(default_factory=pd.Series)

    # Trade history
    trades: List[TradeResult] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            'total_return': self.total_return,
            'annual_return': self.annual_return,
            'sharpe_ratio': self.sharpe_ratio,
            'sortino_ratio': self.sortino_ratio,
            'max_drawdown': self.max_drawdown,
            'max_drawdown_duration': self.max_drawdown_duration,
            'calmar_ratio': self.calmar_ratio,
            'total_trades': self.total_trades,
            'winning_trades': self.winning_trades,
            'losing_trades': self.losing_trades,
            'win_rate': self.win_rate,
            'avg_win': self.avg_win,
            'avg_loss': self.avg_loss,
            'profit_factor': self.profit_factor,
            'avg_trade_duration': self.avg_trade_duration,
            'volatility': self.volatility,
            'var_95': self.var_95,
            'cvar_95': self.cvar_95
        }


@dataclass
class WalkForwardResult:
    """Results from walk-forward analysis."""
    # Individual fold results
    in_sample_results: List[BacktestResult]
    out_of_sample_results: List[BacktestResult]

    # Aggregated metrics
    avg_is_sharpe: float
    avg_oos_sharpe: float
    walk_forward_efficiency: float  # OOS Sharpe / IS Sharpe

    # Combined OOS  equity
    combined_oos_return: float
    combined_oos_sharpe: float
    combined_oos_max_dd: float

    # Fold details
    fold_dates: List[Tuple[datetime, datetime, datetime]]  # (train_start, train_end, test_end)

    def is_robust(self) -> bool:
        """Check if strategy passes robustness tests."""
        return (
            self.walk_forward_efficiency > 0.5 and  # WFE > 50%
            self.combined_oos_sharpe > 0.5 and  # Positive OOS Sharpe
            self.combined_oos_max_dd > -0.30  # Max DD < 30%
        )


# =============================================================================
# BACKTEST ENGINE
# =============================================================================
class BacktestEngine:
    """
    Professional backtesting engine using VectorBT.

    Features:
    - Vectorized operations (100x faster than loops)
    - Realistic transaction costs and slippage
    - Walk-forward optimization for robustness testing
    - Multiple order types and position management
    """

    def __init__(self, data: pd.DataFrame, config: Optional[BacktestConfig] = None):
        """
        Initialize backtest engine.

        Args:
            data: DataFrame with OHLCV + signals (must have 'Signal' column)
            config: Backtest configuration (uses defaults if None)
        """
        self.data = data.copy()
        self.config = config or BacktestConfig()
        self._validate_data()

        # VectorBT portfolio (set after running backtest)
        self.portfolio = None
        self.results = None

        # Initialize PositionSizer if enabled
        self.position_sizer = None
        if self.config.use_position_sizer:
            self.position_sizer = PositionSizer(
                target_volatility=self.config.target_volatility,
                max_portfolio_heat=self.config.max_portfolio_heat,
                max_per_trade_risk=self.config.max_per_trade_risk,
                kelly_fraction=self.config.kelly_fraction
            )
            # Fit GARCH model on historical returns
            returns = self.data['Close'].pct_change().dropna()
            if len(returns) > 50:
                self.position_sizer.fit_garch(returns)

    def _validate_data(self):
        """Validate required columns exist."""
        required = ['Open', 'High', 'Low', 'Close', 'Volume']
        missing = [col for col in required if col not in self.data.columns]
        if missing:
            raise ValueError(f"Missing required columns: {missing}")

        if 'Signal' not in self.data.columns:
            raise ValueError("Missing 'Signal' column. Run SignalGenerator first.")

    # =========================================================================
    # SIGNAL CONVERSION
    # =========================================================================
    def _convert_signals_to_entries_exits(self) -> Tuple[pd.Series, pd.Series]:
        """
        Convert signal strings to boolean entry/exit arrays.

        Signal mapping:
        - STRONG_BUY, BUY → Long entry
        - STRONG_SELL, SELL → Long exit (or short entry if enabled)
        - HOLD → No action

        Returns:
            Tuple of (entries, exits) boolean Series
        """
        signals = self.data['Signal'].values
        n = len(signals)

        entries = np.zeros(n, dtype=bool)
        exits = np.zeros(n, dtype=bool)

        # Entry on buy signals
        buy_signals = (signals == 'STRONG_BUY') | (signals == 'BUY')
        # Exit on sell signals
        sell_signals = (signals == 'STRONG_SELL') | (signals == 'SELL')

        # Simple state machine: enter on buy, exit on sell
        in_position = False
        for i in range(n):
            if not in_position and buy_signals[i]:
                entries[i] = True
                in_position = True
            elif in_position and sell_signals[i]:
                exits[i] = True
                in_position = False

        return pd.Series(entries, index=self.data.index), pd.Series(exits, index=self.data.index)

    def _get_position_sizes(self, data: Optional[pd.DataFrame] = None) -> pd.Series:
        """
        Calculate position sizes using PositionSizer (Kelly, GARCH, vol targeting).

        If PositionSizer is enabled, uses:
        - Kelly Criterion (Quarter-Kelly) for optimal sizing
        - GARCH(1,1) volatility forecasting
        - Volatility targeting (15% annualized)
        - Risk budgeting (max portfolio heat, per-trade risk)
        - Drawdown-based scaling

        Falls back to simple confidence-based sizing if PositionSizer disabled.
        """
        if data is None:
            data = self.data
        signals = data['Signal'].values
        n = len(data)

        if self.position_sizer is not None:
            # Use PositionSizer for dynamic sizing
            returns = data['Close'].pct_change().dropna()
            sizes = np.zeros(n)

            # Calculate equity curve and drawdown for position scaling
            equity = (1 + returns).cumprod()
            drawdown = (equity / equity.cummax() - 1).abs()

            # Get trade history if available (for Kelly calculation)
            trade_history = None
            if self.portfolio is not None and hasattr(self.portfolio, 'trades'):
                try:
                    trades = self.portfolio.trades.records_readable
                    if len(trades) >= 10:
                        trade_history = pd.DataFrame({'return': trades['Return'].values})
                except:
                    pass

            # Calculate position size for each bar
            for i in range(len(returns), n):
                hist_returns = returns.iloc[:i] if i > 0 else returns
                if len(hist_returns) < 20:
                    sizes[i] = self.config.position_size_pct
                else:
                    current_dd = drawdown.iloc[i-1] if i > 0 and i-1 < len(drawdown) else 0.0
                    size_pct = self.position_sizer.get_position_size_pct(
                        returns=hist_returns,
                        current_drawdown=current_dd,
                        trade_history=trade_history
                    )
                    sizes[i] = size_pct

            # Fill initial period with base size
            sizes[:len(returns)] = self.config.position_size_pct

            # Apply signal strength adjustment
            is_strong = (signals == 'STRONG_BUY') | (signals == 'STRONG_SELL')
            is_regular = (signals == 'BUY') | (signals == 'SELL')
            sizes[is_regular & ~is_strong] *= 0.7

            return pd.Series(sizes, index=data.index)

        # Fallback: Simple confidence-based sizing (original behavior)
        confidence = data.get('Signal_Confidence', pd.Series(np.ones(n) * 0.5)).values
        sizes = np.ones(n) * self.config.position_size_pct

        # Scale by confidence (0.5-1.0 range → 50%-100% of base size)
        confidence_scale = 0.5 + 0.5 * confidence
        sizes = sizes * confidence_scale

        # Strong signals get full size, regular get 70%
        is_strong = (signals == 'STRONG_BUY') | (signals == 'STRONG_SELL')
        is_regular = (signals == 'BUY') | (signals == 'SELL')

        sizes[is_regular & ~is_strong] *= 0.7

        return pd.Series(sizes, index=data.index)

    def _calculate_slippage(self) -> pd.Series:
        """
        Calculate realistic slippage based on volume.

        Higher volume = lower slippage (more liquidity)
        """
        base_slippage = self.config.slippage_pct

        # Volume-based adjustment
        if 'Volume' in self.data.columns:
            volume = self.data['Volume'].values
            avg_volume = np.mean(volume)

            # Low volume = higher slippage
            volume_ratio = volume / avg_volume
            volume_adjustment = self.config.volume_impact * (1 / np.maximum(volume_ratio, 0.1) - 1)
            volume_adjustment = np.clip(volume_adjustment, 0, 0.01)  # Cap at 1%

            slippage = base_slippage + volume_adjustment
        else:
            slippage = np.full(len(self.data), base_slippage)

        return pd.Series(slippage, index=self.data.index)

    # =========================================================================
    # MAIN BACKTEST
    # =========================================================================
    def run_backtest(self, start_date: Optional[str] = None,
                     end_date: Optional[str] = None) -> BacktestResult:
        """
        Run vectorized backtest using VectorBT.

        Args:
            start_date: Optional start date filter
            end_date: Optional end date filter

        Returns:
            BacktestResult with all performance metrics
        """
        # Filter date range
        data = self.data.copy()
        if start_date:
            data = data[data.index >= start_date]
        if end_date:
            data = data[data.index <= end_date]

        if len(data) < self.config.min_test_periods:
            raise ValueError(f"Insufficient data: {len(data)} rows, need {self.config.min_test_periods}")

        # Store filtered data for this run
        self._current_data = data

        # Convert signals to entries/exits
        entries, exits = self._convert_signals_to_entries_exits()
        entries = entries.loc[data.index]
        exits = exits.loc[data.index]

        # Get prices
        close = data['Close']

        # Calculate stop loss and take profit levels if enabled
        sl_stop = None
        tp_stop = None

        if self.config.use_stop_loss and 'ATR' in data.columns:
            atr = data['ATR'].values
            sl_stop = self.config.stop_loss_atr_mult * atr / close.values #Exit if price drops by (2 × ATR) as a percentage of price.

        if self.config.use_take_profit and 'ATR' in data.columns:
            atr = data['ATR'].values
            tp_stop = self.config.take_profit_atr_mult * atr / close.values

        # Calculate position sizes (uses PositionSizer if enabled)
        position_sizes = self._get_position_sizes(data)

        # Run VectorBT portfolio simulation
        self.portfolio = vbt.Portfolio.from_signals(
            close=close,
            entries=entries,
            exits=exits,
            size=position_sizes,  # Dynamic position sizing
            size_type='percent',  # Size as percentage of equity
            init_cash=self.config.initial_capital,
            fees=self.config.commission_pct,
            slippage=self.config.slippage_pct,
            sl_stop=sl_stop,
            tp_stop=tp_stop,
            freq='1D'
        )

        # Extract results
        self.results = self._extract_results()
        return self.results

    def _extract_results(self) -> BacktestResult:
        """Extract comprehensive results from VectorBT portfolio."""
        pf = self.portfolio

        # Basic returns
        total_return = float(pf.total_return())

        # Annualized metrics
        returns = pf.returns()
        annual_return = float(returns.mean() * 252) if len(returns) > 0 else 0
        volatility = float(returns.std() * np.sqrt(252)) if len(returns) > 0 else 0

        # Risk-adjusted returns using PerformanceAnalyser for consistency with llm_agent.py
        # Both files now use the same calculation methodology (5% annual risk-free rate)
        try:
            trades_df = pf.trades.records_readable if hasattr(pf.trades, 'records_readable') else None
        except:
            trades_df = None
        analyser = PerformanceAnalyser(returns, trades_df)
        sharpe = analyser.calculate_sharpe_ratio()
        sortino = analyser.calculate_sortino_ratio()

        # Drawdown
        max_dd = float(pf.max_drawdown())
        try:
            dd_duration = int(pf.drawdowns.max_duration()) if len(pf.drawdowns.records) > 0 else 0
        except:
            dd_duration = 0
        calmar = annual_return / abs(max_dd) if max_dd != 0 else 0

        # Trade statistics
        trades = pf.trades.records_readable if hasattr(pf.trades, 'records_readable') else pd.DataFrame()
        total_trades = len(trades) if len(trades) > 0 else 0

        if total_trades > 0:
            winning = trades[trades['PnL'] > 0]
            losing = trades[trades['PnL'] < 0]

            winning_trades = len(winning)
            losing_trades = len(losing)
            win_rate = winning_trades / total_trades

            avg_win = float(winning['PnL'].mean()) if len(winning) > 0 else 0
            avg_loss = float(losing['PnL'].mean()) if len(losing) > 0 else 0

            gross_profit = winning['PnL'].sum() if len(winning) > 0 else 0
            gross_loss = abs(losing['PnL'].sum()) if len(losing) > 0 else 0
            profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')

            # Average trade duration
            if 'Exit Timestamp' in trades.columns and 'Entry Timestamp' in trades.columns:
                durations = (pd.to_datetime(trades['Exit Timestamp']) -
                           pd.to_datetime(trades['Entry Timestamp'])).dt.days
                avg_duration = float(durations.mean())
            else:
                avg_duration = 0
        else:
            winning_trades = losing_trades = 0
            win_rate = avg_win = avg_loss = 0
            profit_factor = 0
            avg_duration = 0

        # Risk metrics (VaR and CVaR)
        if len(returns) > 0:
            var_95 = float(np.percentile(returns, 5))
            cvar_95 = float(returns[returns <= var_95].mean()) if len(returns[returns <= var_95]) > 0 else var_95
        else:
            var_95 = cvar_95 = 0

        # Equity and drawdown curves
        equity_curve = pf.value()
        drawdown_curve = pf.drawdown()

        return BacktestResult(
            total_return=total_return,
            annual_return=annual_return,
            sharpe_ratio=sharpe,
            sortino_ratio=sortino,
            max_drawdown=max_dd,
            max_drawdown_duration=dd_duration,
            calmar_ratio=calmar,
            total_trades=total_trades,
            winning_trades=winning_trades,
            losing_trades=losing_trades,
            win_rate=win_rate,
            avg_win=avg_win,
            avg_loss=avg_loss,
            profit_factor=profit_factor,
            avg_trade_duration=avg_duration,
            volatility=volatility,
            var_95=var_95,
            cvar_95=cvar_95,
            equity_curve=equity_curve,
            drawdown_curve=drawdown_curve
        )

    # =========================================================================
    # WALK-FORWARD OPTIMIZATION
    # =========================================================================
    def run_walk_forward(self, n_folds: int = 5) -> WalkForwardResult:
        """
        Run walk-forward analysis for robustness testing.

        Walk-forward process:
        1. Split data into n_folds sequential periods
        2. For each fold, train on previous data, test on current fold
        3. Calculate Walk-Forward Efficiency = OOS Sharpe / IS Sharpe

        A WFE > 0.5 indicates the strategy is robust (not overfit).

        Args:
            n_folds: Number of out-of-sample folds

        Returns:
            WalkForwardResult with all fold statistics
        """
        data = self.data.copy()
        n = len(data)

        # Calculate fold sizes based on walk-forward ratio
        # Total periods = IS + OOS, where IS = ratio * OOS
        # So OOS = Total / (1 + ratio)
        oos_size = int(n // (n_folds + self.config.walk_forward_ratio))
        is_size = int(oos_size * self.config.walk_forward_ratio)

        # Ensure minimum periods
        oos_size = max(oos_size, self.config.min_test_periods)
        is_size = max(is_size, self.config.min_train_periods)

        is_results = []
        oos_results = []
        fold_dates = []

        print(f"\n Running Walk-Forward Analysis ({n_folds} folds)...")
        print(f"  In-sample size: ~{is_size} days")
        print(f"  Out-of-sample size: ~{oos_size} days")

        for fold in range(n_folds):
            # Calculate fold boundaries
            # OOS period for this fold
            oos_end_idx = n - (n_folds - fold - 1) * oos_size
            oos_start_idx = oos_end_idx - oos_size

            # IS period is everything before OOS (up to is_size * some factor)
            is_end_idx = oos_start_idx
            is_start_idx = max(0, is_end_idx - is_size - fold * oos_size)  # Expanding window

            if is_end_idx - is_start_idx < self.config.min_train_periods:
                print(f"  Fold {fold + 1}: Skipped (insufficient training data)")
                continue

            # Get date ranges
            is_start = data.index[is_start_idx]
            is_end = data.index[is_end_idx - 1]
            oos_end = data.index[min(oos_end_idx - 1, n - 1)]

            fold_dates.append((is_start, is_end, oos_end))

            print(f"  Fold {fold + 1}: Train {is_start.date()} to {is_end.date()} | Test to {oos_end.date()}")

            # Run IS backtest
            try:
                is_result = self.run_backtest(
                    start_date=str(is_start.date()),
                    end_date=str(is_end.date())
                )
                is_results.append(is_result)
            except Exception as e:
                print(f"    IS backtest failed: {e}")
                continue

            # Run OOS backtest
            try:
                oos_result = self.run_backtest(
                    start_date=str(data.index[oos_start_idx].date()),
                    end_date=str(oos_end.date())
                )
                oos_results.append(oos_result)
            except Exception as e:
                print(f"    OOS backtest failed: {e}")
                continue

        if len(is_results) == 0 or len(oos_results) == 0:
            raise ValueError("Walk-forward analysis failed: no valid folds")

        # Calculate aggregate metrics
        avg_is_sharpe = np.mean([r.sharpe_ratio for r in is_results])
        avg_oos_sharpe = np.mean([r.sharpe_ratio for r in oos_results])

        # Walk-Forward Efficiency
        wfe = avg_oos_sharpe / avg_is_sharpe if avg_is_sharpe > 0 else 0

        # Combined OOS metrics (chain all OOS periods)
        combined_oos_return = np.prod([1 + r.total_return for r in oos_results]) - 1
        combined_oos_sharpe = avg_oos_sharpe  # Approximation
        combined_oos_max_dd = min([r.max_drawdown for r in oos_results])

        result = WalkForwardResult(
            in_sample_results=is_results,
            out_of_sample_results=oos_results,
            avg_is_sharpe=avg_is_sharpe,
            avg_oos_sharpe=avg_oos_sharpe,
            walk_forward_efficiency=wfe,
            combined_oos_return=combined_oos_return,
            combined_oos_sharpe=combined_oos_sharpe,
            combined_oos_max_dd=combined_oos_max_dd,
            fold_dates=fold_dates
        )

        print(f"\n Walk-Forward Results:")
        print(f"  Avg IS Sharpe: {avg_is_sharpe:.2f}")
        print(f"  Avg OOS Sharpe: {avg_oos_sharpe:.2f}")
        print(f"  Walk-Forward Efficiency: {wfe:.1%}")
        print(f"  Robustness Check: {'PASS' if result.is_robust() else 'FAIL'}")

        return result

    # =========================================================================
    # BENCHMARK COMPARISON
    # =========================================================================
    def compare_to_benchmark(self, benchmark: str = 'buy_hold', spy_data: pd.DataFrame = None) -> Dict[str, Any]:
        """
        Compare strategy to benchmark.

        Args:
            benchmark: 'buy_hold' or 'spy'
            spy_data: Required if benchmark='spy'. DataFrame with SPY price data (must have 'Close' column)

        Returns:
            Dictionary with comparison metrics
        """
        if self.results is None:
            raise ValueError("Run backtest first")

        data = self._current_data if hasattr(self, '_current_data') else self.data

        if benchmark == 'buy_hold':
            # Buy & hold: Buy on day 1, sell on last day (baseline for single-stock strategy)
            bh_return = (data['Close'].iloc[-1] / data['Close'].iloc[0]) - 1
            bh_returns = data['Close'].pct_change().dropna()
            # Use 5% risk-free rate to match PerformanceAnalyser
            daily_rf = (1 + 0.05) ** (1/252) - 1
            bh_sharpe = ((bh_returns.mean() - daily_rf) * 252) / (bh_returns.std() * np.sqrt(252))
            bh_max_dd = (data['Close'] / data['Close'].cummax() - 1).min()

            benchmark_metrics = {
                'total_return': bh_return,
                'sharpe_ratio': bh_sharpe,
                'max_drawdown': bh_max_dd
            }

        elif benchmark == 'spy':
            # SPY benchmark: Compare against S&P 500 index (did we beat the market?)
            if spy_data is None:
                raise ValueError("spy_data required for SPY benchmark. Use DataCollector to fetch SPY data.")

            if 'Close' not in spy_data.columns:
                raise ValueError("spy_data must have 'Close' column")

            # Align SPY data to strategy date range
            start_date = data.index[0]
            end_date = data.index[-1]
            spy_aligned = spy_data.loc[start_date:end_date]

            if len(spy_aligned) < 2:
                raise ValueError(f"Insufficient SPY data for date range {start_date} to {end_date}")

            # Calculate SPY metrics (same formulas as buy_hold)
            spy_return = (spy_aligned['Close'].iloc[-1] / spy_aligned['Close'].iloc[0]) - 1
            spy_returns = spy_aligned['Close'].pct_change().dropna()
            # Use 5% risk-free rate to match PerformanceAnalyser
            daily_rf = (1 + 0.05) ** (1/252) - 1
            spy_sharpe = ((spy_returns.mean() - daily_rf) * 252) / (spy_returns.std() * np.sqrt(252))
            spy_max_dd = (spy_aligned['Close'] / spy_aligned['Close'].cummax() - 1).min()

            benchmark_metrics = {
                'total_return': spy_return,
                'sharpe_ratio': spy_sharpe,
                'max_drawdown': spy_max_dd
            }

        else:
            raise ValueError(f"Unknown benchmark: {benchmark}. Use 'buy_hold' or 'spy'")

        # Calculate alpha and beta (CAPM metrics)
        # Alpha = strategy excess return not explained by market exposure
        # Beta = strategy sensitivity to benchmark movements
        strategy_returns = self.portfolio.returns()

        # Use SPY returns for alpha/beta if SPY benchmark, otherwise use underlying asset
        if benchmark == 'spy':
            benchmark_returns = spy_aligned['Close'].pct_change().dropna()
        else:
            benchmark_returns = data['Close'].pct_change().dropna()

        # Align strategy and benchmark returns to same dates
        common_idx = strategy_returns.index.intersection(benchmark_returns.index)
        strat_ret = strategy_returns.loc[common_idx]
        bench_ret = benchmark_returns.loc[common_idx]

        if len(common_idx) > 1:
            # Beta = Cov(strategy, benchmark) / Var(benchmark)
            cov_matrix = np.cov(strat_ret, bench_ret)
            beta = cov_matrix[0, 1] / cov_matrix[1, 1] if cov_matrix[1, 1] != 0 else 1
            # Alpha = annualized excess return after adjusting for beta
            alpha = (strat_ret.mean() - beta * bench_ret.mean()) * 252
        else:
            beta = 1
            alpha = 0

        return {
            'strategy': self.results.to_dict(),
            'benchmark': benchmark_metrics,
            'benchmark_name': benchmark,  # Include which benchmark was used
            'alpha': alpha,
            'beta': beta,
            'excess_return': self.results.total_return - benchmark_metrics['total_return'],
            'excess_sharpe': self.results.sharpe_ratio - benchmark_metrics['sharpe_ratio']
        }

    # =========================================================================
    # OUTPUT METHODS
    # =========================================================================
    def get_summary(self) -> str:
        """Get formatted summary of backtest results."""
        if self.results is None:
            return "No backtest results. Run backtest first."

        r = self.results

        summary = f"""
        === BACKTEST SUMMARY ===

        PERFORMANCE:
        Total Return: {r.total_return:.1%}
        Annual Return: {r.annual_return:.1%}
        Volatility: {r.volatility:.1%}

        RISK-ADJUSTED:
        Sharpe Ratio: {r.sharpe_ratio:.2f}
        Sortino Ratio: {r.sortino_ratio:.2f}
        Calmar Ratio: {r.calmar_ratio:.2f}

        DRAWDOWN:
        Max Drawdown: {r.max_drawdown:.1%}
        Max DD Duration: {r.max_drawdown_duration} days

        TRADES:
        Total Trades: {r.total_trades}
        Win Rate: {r.win_rate:.1%}
        Profit Factor: {r.profit_factor:.2f}
        Avg Trade Duration: {r.avg_trade_duration:.1f} days

        RISK METRICS:
        VaR (95%): {r.var_95:.2%}
        CVaR (95%): {r.cvar_95:.2%}
        """
        return summary

    def get_trade_analysis(self) -> pd.DataFrame:
        """Get detailed trade-by-trade analysis."""
        if self.portfolio is None:
            return pd.DataFrame()

        trades = self.portfolio.trades.records_readable
        if len(trades) == 0:
            return pd.DataFrame()

        return trades


# =============================================================================
# TEST SCRIPT
# =============================================================================
if __name__ == "__main__":
    from data_collector import DataCollector
    from technical_indicators import TechnicalIndicators
    from regime_detector import RegimeDetector
    from signal_generator import SignalGenerator

    # Load and prepare data
    print("Loading data...")
    collector = DataCollector()
    data = collector.get_data('AAPL', years=10)

    print("Calculating indicators...")
    ti = TechnicalIndicators(data)
    data = ti.calculate_all()

    print("Detecting regimes...")
    rd = RegimeDetector(data)
    data = rd.detect_all_regimes()

    print("Generating signals...")
    sg = SignalGenerator(data)
    data = sg.generate_signals()

    # Run backtest
    print("\n" + "="*50)
    print("RUNNING BACKTEST")
    print("="*50)

    config = BacktestConfig(
        initial_capital=100_000,
        commission_pct=0.001,
        use_stop_loss=True,
        use_take_profit=True
    )

    engine = BacktestEngine(data, config)
    results = engine.run_backtest()

    print(engine.get_summary())

    # Compare to buy & hold
    print("\n" + "="*50)
    print("BENCHMARK COMPARISON (Buy & Hold)")
    print("="*50)
    comparison = engine.compare_to_benchmark('buy_hold')
    print(f"Strategy Return: {comparison['strategy']['total_return']:.1%}")
    print(f"Buy & Hold Return: {comparison['benchmark']['total_return']:.1%}")
    print(f"Excess Return: {comparison['excess_return']:.1%}")
    print(f"Alpha: {comparison['alpha']:.2%}")
    print(f"Beta: {comparison['beta']:.2f}")

    # Compare to SPY (S&P 500)
    # Load SPY data same way as AAPL - from cache if available, otherwise fetch
    print("\n" + "="*50)
    print("BENCHMARK COMPARISON (SPY)")
    print("="*50)
    # get_data() loads from cache if available, downloads if not
    spy_data = collector.get_data('SPY', years=10)

    spy_comparison = engine.compare_to_benchmark('spy', spy_data=spy_data)
    print(f"Strategy Return: {spy_comparison['strategy']['total_return']:.1%}")
    print(f"SPY Return: {spy_comparison['benchmark']['total_return']:.1%}")
    print(f"Excess Return (vs market): {spy_comparison['excess_return']:.1%}")
    print(f"Alpha (vs market): {spy_comparison['alpha']:.2%}")
    print(f"Beta (market sensitivity): {spy_comparison['beta']:.2f}")

    # Walk-forward analysis
    print("\n" + "="*50)
    print("WALK-FORWARD ANALYSIS")
    print("="*50)
    wf_results = engine.run_walk_forward(n_folds=4)

    print(f"\nRobustness: {'PASS - Strategy is robust' if wf_results.is_robust() else 'FAIL - Strategy may be overfit'}")
